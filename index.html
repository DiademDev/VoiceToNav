<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Wayfinding Demo</title>
  <style>
    html, body {
      margin: 0;
      padding: 0;
      height: 100%;
      overflow: hidden;
    }
    #video {
      position: fixed;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover; /* ensures camera feed covers the screen */
      z-index: -1; /* keeps it in the background */
    }
    .overlay {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      padding: 1rem;
      color: white;
      text-shadow: 1px 1px 2px black;
      z-index: 1;
    }
    button {
      padding: 0.6rem 1rem;
      font-size: 1rem;
      border: none;
      border-radius: 0.5rem;
      margin: 0.5rem;
      background: rgba(0,0,0,0.6);
      color: white;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <!-- ðŸ“· Fullscreen Camera -->
  <video id="video" autoplay playsinline></video>

  <!-- UI Overlay -->
  <div class="overlay">
    <h2>Wayfinding Prototype</h2>
    <button id="start-voice">Ask a Question</button>
    <p id="voice-text"></p>
    
    <button id="capture">Scan Directory</button>
    <p id="ocr-text"></p>
    <p id="result"></p>
  </div>

  <canvas id="canvas" hidden></canvas>
  
  <script src="https://cdn.jsdelivr.net/npm/tesseract.js@5/dist/tesseract.min.js"></script>
  <script>
    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");

    // ðŸ“· Start Camera (back-facing / environment)
    navigator.mediaDevices.getUserMedia({
      video: { facingMode: { exact: "environment" } }
    }).then(stream => {
      video.srcObject = stream;
    }).catch(err => {
      console.error("Camera error:", err);
    });

    // ðŸŽ¤ Voice Input
    document.getElementById("start-voice").onclick = () => {
      const recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
      recognition.lang = "en-US";
      recognition.start();
      recognition.onresult = (e) => {
        const transcript = e.results[0][0].transcript;
        document.getElementById("voice-text").innerText = "You asked: " + transcript;
        window.userQuery = transcript.toLowerCase();
      };
    };

    // ðŸ“· Capture + OCR
    document.getElementById("capture").onclick = async () => {
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      ctx.drawImage(video, 0, 0);
      const dataUrl = canvas.toDataURL("image/png");

      const result = await Tesseract.recognize(dataUrl, "eng");
      const ocrText = result.data.text.toLowerCase();
      document.getElementById("ocr-text").innerText = "Board says:\n" + ocrText;

      // ðŸ§  Simple Matching Logic
      let reply = "Sorry, I couldnâ€™t find your destination.";
      if (window.userQuery && ocrText.includes(window.userQuery.split(" ").pop())) {
        reply = `I found ${window.userQuery}. Follow the signs to reach it.`;
      }

      document.getElementById("result").innerText = reply;

      // ðŸ”Š Speak Result
      const utterance = new SpeechSynthesisUtterance(reply);
      speechSynthesis.speak(utterance);
    };
  </script>
</body>
</html>
